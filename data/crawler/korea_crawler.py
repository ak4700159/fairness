from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service as ChromeService
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.support.ui import WebDriverWait
import pandas as pd
import time
import json
import re

# https://www.koreadaily.com/search?searchWord=%EB%AF%BC%EC%83%9D+%EC%A0%95%EC%B1%85&searchType=all&highLight=true&page=1
class KoreaCrawler:
    def __init__(self, keyword, max_pages=5):
        self.keyword = keyword
        self.max_pages = max_pages
        self.driver = None
        self.wait = None
        self.article_links = []
        self.results = [] # [{"ê¸°ì‚¬ì œëª©" : "ê¸°ì‚¬ì œëª©", "ê¸°ì‚¬ë³¸ë¬¸" : "ê¸°ì‚¬ë³¸ë¬¸"}, {"ê¸°ì‚¬ì œëª©" : "ê¸°ì‚¬ì œëª©", "ê¸°ì‚¬ë³¸ë¬¸" : "ê¸°ì‚¬ë³¸ë¬¸"},]

    # íŒ¨í‚¤ì§€ ë“œë¼ì´ë²„ ì„¸íŒ… 
    def setup_driver(self):
        options = Options()
        options.add_argument('--headless')  # UI ìˆ¨ê¸°ê¸° ì˜µì…˜ ì œê±° 
        options.add_argument('--no-sandbox')
        # options.add_argument('--disable-dev-shm-usage')
        self.driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()),options=options)
        self.wait = WebDriverWait(self.driver, 10)
        print("âœ… WebDriver ì´ˆê¸°í™” ì™„ë£Œ")
    
    # ê¸°ì‚¬ë§í¬ ìˆ˜ì§‘
    def collect_article_links(self):
        for i in range(self.max_pages):
            base_url = f"https://www.koreadaily.com/search?searchWord={self.keyword}&searchType=all&highLight=true&page={i+1}"
            self.driver.get(base_url)
            time.sleep(1.5)
            articles = self.driver.find_elements(By.CSS_SELECTOR, "div.newsList a")
            links = set()
            for article in articles:
                try:
                    href = article.get_attribute("href")
                    if href:
                        links.add(href)
                except:
                    continue
            self.article_links.extend(links)
            print(f"{i+1}í˜ì´ì§€ ë§í¬ ë¦¬ìŠ¤íŠ¸({len(links)}ê°œ) : \n{links}")

    # ê¸°ì‚¬ì œëª©ê³¼ ì›ë³¸ ì¶”ì¶œ
    def extract_article_data(self, url):
        self.driver.get(url)
        try:
            title = self.driver.find_element(By.CSS_SELECTOR, "div.headline h1").text
        except:
            title = ""
        
        try:
            # ê¸°ì‚¬ ë³¸ë¬¸ p íƒœê·¸ ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ
            content = self.driver.find_element(By.CSS_SELECTOR, "section#newsBody").text
            # í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ í›„ \nìœ¼ë¡œ ì—°ê²°
            # content = "\n".join([p.text.strip() for p in paragraphs])
            if content == "" or bool(re.fullmatch(r'\n*', content)): return None
        except:
            return None

        print(f"âœ… ìˆ˜ì§‘ ì™„ë£Œ: {title[:20]}... / {content[:20]}...")
        return {"ê¸°ì‚¬ì œëª©": title, "ê¸°ì‚¬ë³¸ë¬¸": content, "ì‹ ë¬¸ì‚¬" : "ì¤‘ì•™ì¼ë³´", "URL" : url}

    # ê¸°ì‚¬ ìˆ˜ì§‘ í•¨ìˆ˜
    def collect_articles(self):
        for url in self.article_links:
            article = self.extract_article_data(url)
            if article != None:
                self.results.append(article)


    # ìˆ˜ì§‘ëœ ê¸°ì‚¬ ì €ì¥ í•¨ìˆ˜
    def save_to_file(self, save_path=".", file_name="korea_articls.json"):
        if self.results == []:
            print("ìˆ˜ì§‘ëœ ê¸°ì‚¬ ì—†ìŠµë‹ˆë‹¤.")
            return
        full_path = f"{save_path}/{file_name}"
        with open(full_path, "w", encoding="utf-8") as f:
            json.dump(self.results, f, ensure_ascii=False, indent=2)
        print(f"âœ… íŒŒì¼ ì €ì¥ ì™„ë£Œ: {full_path}")
        

    def run(self):
        self.setup_driver()
        self.collect_article_links()
        self.collect_articles()
        self.save_to_file()
        print("ğŸ§¹ ë“œë¼ì´ë²„ ì¢…ë£Œ ë° ì‘ì—… ì™„ë£Œ")

if __name__ == "__main__":
    crawler = KoreaCrawler(keyword="ë¯¼ìƒ ì§€ì›",max_pages=3)
    crawler.run()
