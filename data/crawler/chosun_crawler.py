from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import pandas as pd
import time
import json

# https://www.chosun.com/nsearch/?query={}&page={}&siteid=&sort=1&date_period=&date_start=&date_end=&writer=&field=&emd_word=&expt_word=&opt_chk=false&app_check=0&website=www,chosun&category=
class ChosunCrawler:
    def __init__(self, keyword, max_pages=5):
        self.keyword = keyword
        self.max_pages = max_pages
        self.driver = None
        self.wait = None
        self.article_links = []
        self.results = [] # [{"ê¸°ì‚¬ì œëª©" : "ê¸°ì‚¬ì œëª©", "ê¸°ì‚¬ë³¸ë¬¸" : "ê¸°ì‚¬ë³¸ë¬¸"}, {"ê¸°ì‚¬ì œëª©" : "ê¸°ì‚¬ì œëª©", "ê¸°ì‚¬ë³¸ë¬¸" : "ê¸°ì‚¬ë³¸ë¬¸"},]

    # íŒ¨í‚¤ì§€ ë“œë¼ì´ë²„ ì„¸íŒ… 
    def setup_driver(self):
        options = Options()
        options.add_argument('--headless')  # UI ìˆ¨ê¸°ê¸° ì˜µì…˜ ì œê±° 
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        self.driver = webdriver.Chrome(service=Service(), options=options)
        self.wait = WebDriverWait(self.driver, 10)
        print("âœ… WebDriver ì´ˆê¸°í™” ì™„ë£Œ")
    
    # ê¸°ì‚¬ë§í¬ ìˆ˜ì§‘
    def collect_article_links(self):
        for i in range(self.max_pages):
            base_url = f"https://www.chosun.com/nsearch/?query={self.keyword}&page={i+1}&siteid=&sort=1&date_period=&date_start=&date_end=&writer=&field=&emd_word=&expt_word=&opt_chk=false&app_check=0&website=www,chosun&category="
            self.driver.get(base_url)
            time.sleep(1.5)
            articles = self.driver.find_elements(By.CSS_SELECTOR, "a.text__link.story-card__headline.box--margin-none.text--black.font--primary.h3.text--left")
            links = set()
            for article in articles:
                try:
                    href = article.get_attribute("href")
                    if href:
                        links.add(href)
                except:
                    continue
            self.article_links.extend(links)
            print(f"{i+1}í˜ì´ì§€ ë§í¬ ë¦¬ìŠ¤íŠ¸ : \n{links}")

    # ê¸°ì‚¬ì œëª©ê³¼ ê¸°ì‚¬ë³¸ë¬¸ ì¶”ì¶œ
    def extract_article_data(self, url):
        self.driver.get(url)
        try:
            title = self.driver.find_element(By.CSS_SELECTOR, "h1.article-header__headline").text
        except:
            title = ""
        
        try:
            # ê¸°ì‚¬ ë³¸ë¬¸ p íƒœê·¸ ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ
            contents = self.driver.find_elements(By.CSS_SELECTOR, "p.article-body__content.article-body__content-text")
            # í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ í›„ \nìœ¼ë¡œ ì—°ê²°
            content = "\n".join([p.text.strip() for p in contents])
        except:
            content = ""

        print(f"âœ… ìˆ˜ì§‘ ì™„ë£Œ: {title[:20]}... / {content[:20]}...")
        return {"ê¸°ì‚¬ì œëª©": title, "ê¸°ì‚¬ë³¸ë¬¸": content, "ì‹ ë¬¸ì‚¬" : "ì¡°ì„  ì¼ë³´"}

    # ê¸°ì‚¬ ìˆ˜ì§‘ í•¨ìˆ˜
    def collect_articles(self):
        for url in self.article_links:
            data = self.extract_article_data(url)
            self.results.append(data)

    # ìˆ˜ì§‘ëœ ê¸°ì‚¬ ì €ì¥ í•¨ìˆ˜
    def save_to_file(self, save_path=".", file_name="chosun_articls.json"):
        if self.results == []:
            print("ìˆ˜ì§‘ëœ ê¸°ì‚¬ ì—†ìŠµë‹ˆë‹¤.")
            return
        full_path = f"{save_path}/{file_name}"
        with open(full_path, "w", encoding="utf-8") as f:
            json.dump(self.results, f, ensure_ascii=False, indent=2)
        print(f"âœ… íŒŒì¼ ì €ì¥ ì™„ë£Œ: {full_path}")
        

    def run(self):
        self.setup_driver()
        self.collect_article_links()
        self.collect_articles()
        self.save_to_file()
        print("ğŸ§¹ ë“œë¼ì´ë²„ ì¢…ë£Œ ë° ì‘ì—… ì™„ë£Œ")

if __name__ == "__main__":
    crawler = ChosunCrawler(keyword="ë¯¼ìƒ ì§€ì›",max_pages=5)
    crawler.run()