{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90a58d37",
   "metadata": {},
   "source": [
    "# Embedding\n",
    "임베딩은 LLM, RAG에서 중요한 개념이다. 즉, 자연어를 기계가 이해할 수 데이터로 변환하는 작업을 임베딩이라고 한다. \n",
    "임베딩 모델은 이미 사전에 학습된 모델이며 상황에 맞는 임베딩 모델을 사용하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77b28432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.00776276458054781, 0.03680367395281792, 0.019545823335647583, -0.0196656696498394, 0.017203375697135925]\n",
      "1536\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# OpenAI의 \"text-embedding-3-large\" 모델을 사용하여 임베딩을 생성합니다.\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "text = \"임베딩 테스트를 하기 위한 샘플 문장입니다.\"\n",
    "query_result = embeddings.embed_query(text)\n",
    "print(query_result[:5])\n",
    "print(len(query_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f1a2c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "import json\n",
    "\n",
    "with open(r\"C:\\Users\\cn120\\바탕 화면\\rag\\fairness\\data\\dumy\\total_articls.json\", 'r', encoding='UTF-8') as f:\n",
    "    json_data = json.load(f)\n",
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    # 텍스트를 분할할 때 사용할 구분자를 지정합니다. 기본값은 \"\\n\\n\"입니다.\n",
    "    # separator= \" \",\n",
    "    # 분할된 텍스트 청크의 최대 크기(글자 갯수)를 지정합니다.\n",
    "    chunk_size=3000,\n",
    "    # 분할된 텍스트 청크 간의 중복되는 문자 수를 지정합니다.\n",
    "    chunk_overlap=50,\n",
    "    # 텍스트의 길이를 계산하는 함수를 지정합니다.\n",
    "    length_function=len,\n",
    "    # 구분자가 정규식인지 여부를 지정합니다.\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "\n",
    "embeddings_1024 = OpenAIEmbeddings(model=\"text-embedding-3-small\", dimensions=1024)\n",
    "# text_splitter를 사용하여 state_of_the_union 텍스트를 문서로 분할합니다.\n",
    "texts = text_splitter.create_documents([json_data[i]['기사본문'] for i in range(len(json_data))])\n",
    "doc_result = embeddings_1024.embed_documents(\n",
    "    [text.page_content for text in texts]\n",
    ")  # 텍스트를 임베딩하여 문서 벡터를 생성합니다.\n",
    "# 문서 결과의 첫 번째 요소에서 처음 5개 항목을 선택합니다.\n",
    "# doc_result[1][:5]\n",
    "print(len(doc_result[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c20979b",
   "metadata": {},
   "source": [
    "# 유사도 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c185c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[유사도 0.9644] 안녕하세요? 반갑습니다. \t <=====> \t 안녕하세요? 반갑습니다!\n",
      "[유사도 0.8376] 안녕하세요? 반갑습니다. \t <=====> \t 안녕하세요? 만나서 반가워요.\n",
      "[유사도 0.5042] 안녕하세요? 반갑습니다. \t <=====> \t Hi, nice to meet you.\n",
      "[유사도 0.1362] 안녕하세요? 반갑습니다. \t <=====> \t I like to eat apples.\n",
      "[유사도 0.8142] 안녕하세요? 반갑습니다! \t <=====> \t 안녕하세요? 만나서 반가워요.\n",
      "[유사도 0.4790] 안녕하세요? 반갑습니다! \t <=====> \t Hi, nice to meet you.\n",
      "[유사도 0.1318] 안녕하세요? 반갑습니다! \t <=====> \t I like to eat apples.\n",
      "[유사도 0.5128] 안녕하세요? 만나서 반가워요. \t <=====> \t Hi, nice to meet you.\n",
      "[유사도 0.1409] 안녕하세요? 만나서 반가워요. \t <=====> \t I like to eat apples.\n",
      "[유사도 0.2249] Hi, nice to meet you. \t <=====> \t I like to eat apples.\n"
     ]
    }
   ],
   "source": [
    "sentence1 = \"안녕하세요? 반갑습니다.\"\n",
    "sentence2 = \"안녕하세요? 반갑습니다!\"\n",
    "sentence3 = \"안녕하세요? 만나서 반가워요.\"\n",
    "sentence4 = \"Hi, nice to meet you.\"\n",
    "sentence5 = \"I like to eat apples.\"\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    return dot_product / (norm_vec1 * norm_vec2)\n",
    "\n",
    "def similarity(a, b):\n",
    "    return cosine_similarity(a, b)\n",
    "\n",
    "embeddings_1024 = OpenAIEmbeddings(model=\"text-embedding-3-small\", dimensions=1024)\n",
    "sentences = [sentence1, sentence2, sentence3, sentence4, sentence5]\n",
    "embedded_sentences = embeddings_1024.embed_documents(sentences)\n",
    "for i, sentence in enumerate(embedded_sentences):\n",
    "    for j, other_sentence in enumerate(embedded_sentences):\n",
    "        if i < j:\n",
    "            print(\n",
    "                f\"[유사도 {similarity(sentence, other_sentence):.4f}] {sentences[i]} \\t <=====> \\t {sentences[j]}\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05d8245",
   "metadata": {},
   "source": [
    "# Embedding Caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "302d1395",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Could not import faiss python package. Please install it with `pip install faiss-gpu` (for CUDA supported GPU) or `pip install faiss-cpu` (depending on Python version).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\langchain_community\\vectorstores\\faiss.py:56\u001b[0m, in \u001b[0;36mdependable_faiss_import\u001b[1;34m(no_avx2)\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 56\u001b[0m         \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mfaiss\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'faiss'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 39\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# text_splitter를 사용하여 state_of_the_union 텍스트를 문서로 분할합니다.\u001b[39;00m\n\u001b[0;32m     38\u001b[0m documents \u001b[38;5;241m=\u001b[39m text_splitter\u001b[38;5;241m.\u001b[39mcreate_documents([json_data[i][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m기사본문\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(json_data))])\n\u001b[1;32m---> 39\u001b[0m db \u001b[38;5;241m=\u001b[39m \u001b[43mFAISS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcached_embedder\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\langchain_core\\vectorstores\\base.py:848\u001b[0m, in \u001b[0;36mVectorStore.from_documents\u001b[1;34m(cls, documents, embedding, **kwargs)\u001b[0m\n\u001b[0;32m    845\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(ids):\n\u001b[0;32m    846\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mids\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m ids\n\u001b[1;32m--> 848\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_texts(texts, embedding, metadatas\u001b[38;5;241m=\u001b[39mmetadatas, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\langchain_community\\vectorstores\\faiss.py:1044\u001b[0m, in \u001b[0;36mFAISS.from_texts\u001b[1;34m(cls, texts, embedding, metadatas, ids, **kwargs)\u001b[0m\n\u001b[0;32m   1025\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Construct FAISS wrapper from raw documents.\u001b[39;00m\n\u001b[0;32m   1026\u001b[0m \n\u001b[0;32m   1027\u001b[0m \u001b[38;5;124;03mThis is a user friendly interface that:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1041\u001b[0m \u001b[38;5;124;03m        faiss = FAISS.from_texts(texts, embeddings)\u001b[39;00m\n\u001b[0;32m   1042\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1043\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m embedding\u001b[38;5;241m.\u001b[39membed_documents(texts)\n\u001b[1;32m-> 1044\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__from(\n\u001b[0;32m   1045\u001b[0m     texts,\n\u001b[0;32m   1046\u001b[0m     embeddings,\n\u001b[0;32m   1047\u001b[0m     embedding,\n\u001b[0;32m   1048\u001b[0m     metadatas\u001b[38;5;241m=\u001b[39mmetadatas,\n\u001b[0;32m   1049\u001b[0m     ids\u001b[38;5;241m=\u001b[39mids,\n\u001b[0;32m   1050\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1051\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\langchain_community\\vectorstores\\faiss.py:996\u001b[0m, in \u001b[0;36mFAISS.__from\u001b[1;34m(cls, texts, embeddings, embedding, metadatas, ids, normalize_L2, distance_strategy, **kwargs)\u001b[0m\n\u001b[0;32m    984\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    985\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__from\u001b[39m(\n\u001b[0;32m    986\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    994\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    995\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m FAISS:\n\u001b[1;32m--> 996\u001b[0m     faiss \u001b[38;5;241m=\u001b[39m \u001b[43mdependable_faiss_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    997\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m distance_strategy \u001b[38;5;241m==\u001b[39m DistanceStrategy\u001b[38;5;241m.\u001b[39mMAX_INNER_PRODUCT:\n\u001b[0;32m    998\u001b[0m         index \u001b[38;5;241m=\u001b[39m faiss\u001b[38;5;241m.\u001b[39mIndexFlatIP(\u001b[38;5;28mlen\u001b[39m(embeddings[\u001b[38;5;241m0\u001b[39m]))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\langchain_community\\vectorstores\\faiss.py:58\u001b[0m, in \u001b[0;36mdependable_faiss_import\u001b[1;34m(no_avx2)\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mfaiss\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m     59\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not import faiss python package. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     60\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease install it with `pip install faiss-gpu` (for CUDA supported GPU) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     61\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor `pip install faiss-cpu` (depending on Python version).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     62\u001b[0m     )\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m faiss\n",
      "\u001b[1;31mImportError\u001b[0m: Could not import faiss python package. Please install it with `pip install faiss-gpu` (for CUDA supported GPU) or `pip install faiss-cpu` (depending on Python version)."
     ]
    }
   ],
   "source": [
    "from langchain.storage import LocalFileStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.embeddings import CacheBackedEmbeddings\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "import json\n",
    "\n",
    "# OpenAI 임베딩을 사용하여 기본 임베딩 설정\n",
    "embedding = OpenAIEmbeddings()\n",
    "\n",
    "# 로컬 파일 저장소 설정\n",
    "store = LocalFileStore(\"./cache/\")\n",
    "\n",
    "# 캐시를 지원하는 임베딩 생성\n",
    "cached_embedder = CacheBackedEmbeddings.from_bytes_store(\n",
    "    underlying_embeddings=embedding,\n",
    "    document_embedding_cache=store,\n",
    "    namespace=embedding.model,  # 기본 임베딩과 저장소를 사용하여 캐시 지원 임베딩을 생성\n",
    ")\n",
    "\n",
    "with open(r\"C:\\Users\\cn120\\바탕 화면\\rag\\fairness\\data\\dumy\\total_articls.json\", 'r', encoding='UTF-8') as f:\n",
    "    json_data = json.load(f)\n",
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    # 텍스트를 분할할 때 사용할 구분자를 지정합니다. 기본값은 \"\\n\\n\"입니다.\n",
    "    separator= \" \",\n",
    "    # 분할된 텍스트 청크의 최대 크기(글자 갯수)를 지정합니다.\n",
    "    chunk_size=3000,\n",
    "    # 분할된 텍스트 청크 간의 중복되는 문자 수를 지정합니다.\n",
    "    chunk_overlap=50,\n",
    "    # 텍스트의 길이를 계산하는 함수를 지정합니다.\n",
    "    length_function=len,\n",
    "    # 구분자가 정규식인지 여부를 지정합니다.\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "\n",
    "# text_splitter를 사용하여 state_of_the_union 텍스트를 문서로 분할합니다.\n",
    "documents = text_splitter.create_documents([json_data[i]['기사본문'] for i in range(len(json_data))])\n",
    "db = FAISS.from_documents(documents, cached_embedder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fairness",
   "language": "python",
   "name": "fairness"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
